{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBInUKnyo968jvgh+CZApg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fasuyaaaPNG/Tildha.ai/blob/main/Tildha_ai_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dependensi"
      ],
      "metadata": {
        "id": "FRR05YCpCrZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkFgkjfDobDf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "restart kernel"
      ],
      "metadata": {
        "id": "dMQij6LKU_ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quit()"
      ],
      "metadata": {
        "id": "7O3sDLhfU8Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1\n",
        "\n",
        "///////////// Base AI Respon"
      ],
      "metadata": {
        "id": "lez_P5S1CoKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "\n",
        "# Log in to Hugging Face\n",
        "login(\"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\")\n",
        "\n",
        "# Load tokenizer and model with optimizations\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Lvyn/AI-Tildha-Merged\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Lvyn/AI-Tildha-Merged\",\n",
        "    torch_dtype=torch.float16,  # Use FP16 precision\n",
        "    low_cpu_mem_usage=True  # Reduce CPU memory usage\n",
        ")\n",
        "\n",
        "tildha_prompt = \"\"\"Below are instructions explaining the task, paired with feedback that provides further context. Write a response that appropriately completes the request. Your name is Tildha\n",
        "\n",
        "### Request:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    requests = examples[\"request\"]\n",
        "    responses = examples[\"response\"]\n",
        "    texts = []\n",
        "    for request, response in zip(requests, responses):\n",
        "        text = tildha_prompt.format(request, response) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(tildha_prompt.format(prompt, \"\"), return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Model parameters\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        top_k=50,\n",
        "        temperature=1,\n",
        "    )\n",
        "\n",
        "    # Decode and print response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\", 1)[1].strip().replace(\"\", \"\")\n",
        "    return response\n",
        "\n",
        "# Main program\n",
        "while True:\n",
        "    user_input = input(\"Please enter your question (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        quit()\n",
        "    response = generate_response(user_input)\n",
        "    print(\"Response:\", response)\n"
      ],
      "metadata": {
        "id": "ifEhvLSDo816"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1.1\n",
        "\n",
        "///////////// Loop with instance function method"
      ],
      "metadata": {
        "id": "phvbW-RV3voB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "\n",
        "# Log in Hugging Face\n",
        "login(\"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\")\n",
        "\n",
        "# Load tokenizer and model with optimizations\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Lvyn/AI-Tildha-Merged\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Lvyn/AI-Tildha-Merged\",\n",
        "    torch_dtype=torch.float16,  # Use FP16 precision\n",
        "    low_cpu_mem_usage=True  # Reduce CPU memory usage\n",
        ")\n",
        "\n",
        "tildha_prompt = \"\"\"Below are instructions explaining the task, paired with feedback that provides further context. Write a response that appropriately completes the request. Your name is Tildha\n",
        "\n",
        "### Request:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    requests = examples[\"request\"]\n",
        "    responses = examples[\"response\"]\n",
        "    texts = []\n",
        "    for request, response in zip(requests, responses):\n",
        "        text = tildha_prompt.format(request, response) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(tildha_prompt.format(prompt, \"\"), return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Model parameters\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        top_k=50,\n",
        "        temperature=1,\n",
        "    )\n",
        "\n",
        "    # Decode and print response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\", 1)[1].strip().replace(\"\", \"\")\n",
        "    return response\n",
        "\n",
        "# program utama\n",
        "def main():\n",
        "    user_input = input(\"Please enter your question (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "      print(\"Bye\")\n",
        "      quit()\n",
        "    else:\n",
        "      response = generate_response(user_input)\n",
        "      print(\"Response:\", response)\n",
        "      main()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FR-U7u2TI4UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1.2\n",
        "\n",
        "////////////// Menu Program"
      ],
      "metadata": {
        "id": "kyI8si-CeAHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "from IPython.display import clear_output\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "# Log in Hugging Face\n",
        "login(\"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\")\n",
        "\n",
        "# Load tokenizer and model with optimizations\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Lvyn/AI-Tildha-Merged\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Lvyn/AI-Tildha-Merged\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True  # Reduce CPU memory usage\n",
        ")\n",
        "\n",
        "tildha_prompt = \"\"\"Below are the questions users have asked you. Write a response that answers the question appropriately. Answer based on the data you have studied\n",
        "\n",
        "### Request:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    requests = examples[\"request\"]\n",
        "    responses = examples[\"response\"]\n",
        "    texts = []\n",
        "    for request, response in zip(requests, responses):\n",
        "        text = tildha_prompt.format(request, response) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(\n",
        "        tildha_prompt.format(prompt, \"\"), return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Model parameters\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        top_k=50,\n",
        "        temperature=1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and print response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.split(\"### Response:\")[1].strip()\n",
        "    return response\n",
        "\n",
        "def menu():\n",
        "    print(\"\"\"\n",
        "Select mode Tildha AI\n",
        "1. Text to text\n",
        "2. Text to speech\n",
        "3. Speech to text\n",
        "4. Speech to speech\n",
        "5. Exit\n",
        "    \"\"\")\n",
        "    try:\n",
        "        menu_select = int(input(\"Select mode (1/2/3/4/5): \"))\n",
        "        return menu_select\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid input! Select range 1-5\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nOperation canceled by user. Exiting...\")\n",
        "        quit()\n",
        "\n",
        "def text_to_text():\n",
        "    while True:\n",
        "        user_input = input(\"Enter your question (or type 'back' to menu): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"bye\")\n",
        "            sys.exit(0)\n",
        "        elif user_input.lower() == 'back':\n",
        "            clear_output()\n",
        "            main()\n",
        "        else:\n",
        "            response = generate_response(user_input)\n",
        "            print(f\"Response: {response}\\n\")\n",
        "\n",
        "# program utama\n",
        "def main():\n",
        "    clear_output()\n",
        "    while True:\n",
        "        menu_select = menu()\n",
        "        clear_output()\n",
        "        if menu_select == 1:\n",
        "            print(\"Text to text\\n\")\n",
        "            text_to_text()\n",
        "        elif menu_select == 2:\n",
        "            print(\"Text to speech not implemented yet.\")\n",
        "        elif menu_select == 3:\n",
        "            print(\"Speech to text not implemented yet.\")\n",
        "        elif menu_select == 4:\n",
        "            print(\"Speech to speech not implemented yet.\")\n",
        "        elif menu_select == 5:\n",
        "            print(\"Bye\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid input! Select range 1-5\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_5Dt5KJFeETs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}